<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="目录 一.续opencv——级联分类器 二.c语言实现进化算法 三.LeNet 网络实现 MNIST手写数字识别 四.the simplest neural networkmodel 五.Multi-Layered Perceptron 1.Gradient DescentOptimization（梯度下降算法） [2.Multi-Layered Perceptrons andBackpropag">
<meta property="og:type" content="article">
<meta property="og:title" content="AI学习笔记">
<meta property="og:url" content="http://example.com/2024/03/29/AI%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Switch">
<meta property="og:description" content="目录 一.续opencv——级联分类器 二.c语言实现进化算法 三.LeNet 网络实现 MNIST手写数字识别 四.the simplest neural networkmodel 五.Multi-Layered Perceptron 1.Gradient DescentOptimization（梯度下降算法） [2.Multi-Layered Perceptrons andBackpropag">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/621e77f5fb5595bedf41e588670f4c80.png">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/11de84ee5edc6ce0c8007f9604f71c73.png">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/1760fe528375040727919cbb4587890d.png">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/98446f8f75153030ff57d774b4684b02.png">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/82d7932c55daa1260a98b128ecb829ba.png">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/28b6864aabe5cda4e0786e846d3f43df.png">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/093e6e1eb8cb98c2846ce0e7fac93586.png">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/2f4b2ad0f83c2804efe05f9563c4c85f.png">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/44ff93f6ec92975a705328447d9fcb28.png">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/bcc3707448e65ae4040d2b305dc71a12.png">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/9f0836ab887616dbe48bcedfff0a5ea8.png">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/aa5a7f818f593a302d689193dc8bf872.png">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/0f2a5124db743b6101b0841143d3f591.png">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/94e98f86a86028bb9b2d14b0ed7f50d3.png">
<meta property="og:image" content="https://i-blog.csdnimg.cn/blog_migrate/9c3db5d1878b3ecef22e81631f39e2b0.png">
<meta property="article:published_time" content="2024-03-29T09:28:37.000Z">
<meta property="article:modified_time" content="2025-02-08T03:45:06.129Z">
<meta property="article:author" content="SWQ">
<meta property="article:tag" content="opencv">
<meta property="article:tag" content="c++">
<meta property="article:tag" content="python">
<meta property="article:tag" content="学习">
<meta property="article:tag" content="笔记">
<meta property="article:tag" content="人工智能">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i-blog.csdnimg.cn/blog_migrate/621e77f5fb5595bedf41e588670f4c80.png">

<link rel="canonical" href="http://example.com/2024/03/29/AI%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>AI学习笔记 | Switch</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Switch</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">27</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">41</span></a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2024/03/29/AI%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="SWQ">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Switch">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          AI学习笔记
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-03-29 17:28:37" itemprop="dateCreated datePublished" datetime="2024-03-29T17:28:37+08:00">2024-03-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-02-08 11:45:06" itemprop="dateModified" datetime="2025-02-08T11:45:06+08:00">2025-02-08</time>
              </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>39k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>36 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><strong>目录</strong></p>
<p><a href="about:blank#%E4%B8%80.%E7%BB%ADopencv%E2%80%94%E2%80%94%E7%BA%A7%E8%81%94%E5%88%86%E7%B1%BB%E5%99%A8">一.续opencv——级联分类器</a></p>
<p><a href="about:blank#%E4%BA%8C.c%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95">二.c语言实现进化算法</a></p>
<p><a href="about:blank#%E4%B8%89.LeNet%20%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%20MNIST%20%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB">三.LeNet 网络实现 MNIST<br>手写数字识别</a></p>
<p><a href="about:blank#%E5%9B%9B.the%20simplest%20neural%20network%20model">四.the simplest neural network<br>model</a></p>
<p><a href="about:blank#%E4%BA%94.Multi-Layered%20Perceptron">五.Multi-Layered Perceptron</a></p>
<p><a href="about:blank#1.Gradient%20Descent%20Optimization%EF%BC%88%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%EF%BC%89">1.Gradient Descent<br>Optimization（梯度下降算法）</a></p>
<p>[2.Multi-Layered Perceptrons and<br>Backpropagation（多层感知器和反向传播）](about:blank#2.Multi-<br>Layered%20Perceptrons%20and%20Backpropagation%EF%BC%88%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%89)</p>
<p><a href="about:blank#3.%E5%8D%95%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%E6%A8%A1%E5%9E%8B">3.单层感知器模型</a></p>
<p><a href="about:blank#%3C1%3E%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8CX%E4%B8%BA%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%EF%BC%8CY%E4%B8%BA%E6%A0%87%E7%AD%BE%EF%BC%9A">&lt;1&gt;创建数据集，X为特征向量，Y为标签：</a></p>
<p><a href="about:blank#%3C2%3E%C2%A0%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%EF%BC%9A%C2%A0">&lt;2&gt; 前向传播计算过程：
</a></p>
<p><a href="about:blank#%3C3%3E%E4%BD%BF%E7%94%A8softmax%E5%87%BD%E6%95%B0%E8%BD%AC%E6%8D%A2%E4%B8%BA%E6%A6%82%E7%8E%87%EF%BC%9A">&lt;3&gt;使用softmax函数转换为概率：</a></p>
<p><a href="about:blank#%3C4%3E%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0">&lt;4&gt;交叉熵损失函数</a></p>
<p><a href="about:blank#%3C5%3ELoss%20Minimization%20Problem%20and%20Network%20Training%EF%BC%9A">&lt;5&gt;Loss Minimization Problem and Network<br>Training：</a></p>
<p><a href="about:blank#%3C6%3E%E5%87%BD%E6%95%B0%E5%B0%8F%E7%BB%93">&lt;6&gt;函数小结</a></p>
<p><a href="about:blank#%3C7%3ETraining%20the%20Model%C2%A0">&lt;7&gt;Training the Model </a></p>
<p><a href="about:blank#4.%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B">4.网络模型</a></p>
<p><a href="about:blank#%3C1%3E%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C%E7%B1%BB%EF%BC%9A">&lt;1&gt;定义网络类：</a></p>
<p><a href="about:blank#%3C2%3EMulti-Layered%20Models">&lt;2&gt;Multi-Layered Models</a></p>
<p><a href="about:blank#5.%E4%BB%A3%E7%A0%81%E6%95%B4%E5%90%88">5.代码整合</a></p>
<p><a href="about:blank#6.3-layer%20network%20%E5%AE%9E%E7%8E%B0%20mnist%20%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB">6.3-layer network 实现 mnist<br>手写数字识别</a></p>
<p><a href="about:blank#%E5%85%AD.Neural%20Network%20Frameworks">六.Neural Network<br>Frameworks</a></p>
<p><a href="about:blank#1.Keras">1.Keras</a></p>
<p>[&lt;1&gt;Training One-Layer Network<br>(Perceptron)](about:blank#%3C1%3ETraining%20One-<br>Layer%20Network%20%28Perceptron%29)</p>
<p><a href="about:blank#%E2%91%A0%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89">①模型定义</a></p>
<p><a href="about:blank#%E2%91%A1%E6%A8%A1%E5%9E%8B%E7%BC%96%E8%AF%91%EF%BC%88%E6%8C%87%E5%AE%9A%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E3%80%81%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E3%80%90%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AD%89%E3%80%91%E3%80%81%E7%B2%BE%E5%BA%A6%EF%BC%89">②模型编译（指定损失函数、优化方法【梯度下降等】、精度）</a></p>
<p><a href="about:blank#%E2%91%A2%E8%AE%AD%E7%BB%83">③训练</a></p>
<p>[&lt;2&gt;Multi-Class Classificatio（多分类问题）](about:blank#%3C2%3EMulti-<br>Class%20Classificatio%EF%BC%88%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%EF%BC%89)</p>
<p>[&lt;3&gt;Multi-Label Classification（多标签分类）](about:blank#%3C3%3EMulti-<br>Label%20Classification%EF%BC%88%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%EF%BC%89)</p>
<p><a href="about:blank#%3C4%3E%E6%80%BB%E7%BB%93%C2%A0Summary%20of%20Classification%20Loss%20Functions">&lt;4&gt;总结 Summary of Classification Loss<br>Functions</a></p>
<hr>
<p>参考资料：<a target="_blank" rel="noopener" href="https://github.com/microsoft/AI-For-Beginners" title="microsoft&#x2F;AI-For-
Beginners: 12 Weeks, 24 Lessons, AI for All! (github.com)">microsoft&#x2F;AI-For-Beginners: 12 Weeks, 24 Lessons, AI for All!<br>(github.com)</a></p>
<h2 id="一-续opencv——级联分类器"><a href="#一-续opencv——级联分类器" class="headerlink" title="一.续opencv——级联分类器"></a>一.续opencv——级联分类器</h2><p> <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_32971095/article/details/131609797" title="OpenCV学习笔记——《基于OpenCV的数字图像处理》_opencv 数字图像处理-CSDN博客">OpenCV学习笔记——《基于OpenCV的数字图像处理》_opencv 数字图像处理-<br>CSDN博客</a></p>
<pre><code>#include &quot;opencv.hpp&quot;
#include &quot;highgui.hpp&quot;
#include &quot;imgproc.hpp&quot;
#include &lt;iostream&gt;
using namespace cv;
using namespace std;
#pragma comment(lib,&quot;opencv_world480d.lib&quot;)

VideoCapture capture(0);
Mat image;
CascadeClassifier face_cascade;
// 人脸检测
vector&lt;Rect&gt; faces;

int main()
&#123;
    Mat frame_gray;
    face_cascade.load(&quot;OPENCV安装路径/opencv/sources/data/haarcascades/haarcascade_frontalface_alt.xml&quot;);
    while (capture.isOpened())
    &#123;
        capture &gt;&gt; image;
        if (image.empty())break;

        if (waitKey(1) == 27)break;

        // BGR2GRAY
        cvtColor(image, frame_gray, COLOR_BGR2GRAY);

        face_cascade.detectMultiScale(frame_gray, faces);

        for (size_t i = 0; i &lt; faces.size(); i++)
        &#123;
            // 人脸画框
            rectangle(image, faces[i], Scalar(255, 0, 0), 1, 8);
        &#125;

        imshow(&quot;Face detection&quot;,image);

    &#125;
&#125;
</code></pre>
<h2 id="二-c语言实现进化算法"><a href="#二-c语言实现进化算法" class="headerlink" title="二.c语言实现进化算法"></a>二.c语言实现进化算法</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_32971095/article/details/136715239" title="c语言实现进化算法——人工智能导论＜1＞-CSDN博客">c语言实现进化算法——人工智能导论＜1＞-CSDN博客</a></p>
<h2 id="三-LeNet-网络实现-MNIST-手写数字识别"><a href="#三-LeNet-网络实现-MNIST-手写数字识别" class="headerlink" title="三.LeNet 网络实现 MNIST 手写数字识别"></a>三.LeNet 网络实现 MNIST 手写数字识别</h2><p><a target="_blank" rel="noopener" href="https://download.csdn.net/download/qq_32971095/88681364?spm=1001.2014.3001.5501" title="西电计科大三上计算机视觉作业">西电计科大三上计算机视觉作业</a></p>
<h2 id="四-the-simplest-neural-network-model"><a href="#四-the-simplest-neural-network-model" class="headerlink" title="四.the simplest neural network model"></a>四.the simplest neural network model</h2><p>one-layered perceptron, a linear two-class classification model.（单层线性感知机）</p>
<p>Perceptron Model：</p>
<pre><code>    假设我们的模型中有N个特征，在这种情况下，输入向量将是一个大小为N的向量。感知器是一个二元分类模型，即它可以区分两类输入数据。我们将假设对于每个输入向量x，感知器的输出将是+1或-1，这取决于类别。输出将使用以下公式计算:
</code></pre>
<p>y(x) &#x3D; f(wTx)</p>
<p>Training the Perceptron：</p>
<pre><code>    为了训练感知器，我们需要找到一个权重向量w，它能正确地分类大多数值，即产生最小的误差。该误差由感知器准则定义如下:
</code></pre>
<p>E(w) &#x3D; -∑wTxiti</p>
<p>对那些导致错误分类的训练数据点I求和，xi是输入数据，对于负例和正例，ti分别为-1或+1。</p>
<pre><code>    这个标准被认为是权重w的函数，我们需要最小化它。通常，我们使用一种称为梯度下降的方法，在这种方法中，我们从一些初始权重w(0)开始，然后在每一步中根据公式更新权重:
</code></pre>
<p>w(t+1) &#x3D; w(t) - η∇E(w)</p>
<p>这里η是所谓的学习率，∇E(w)表示E的梯度，计算出梯度后，我们得到</p>
<p>w(t+1) &#x3D; w(t) + ∑ηxiti</p>
<pre><code>//perceptron.h
#ifndef _PERCEPTRON_H
#define _PERCEPTRON_H
//the simplest neural network model - one-layered perceptron, a linear two-class classification model.
#include&lt;stdio.h&gt;
#include&lt;time.h&gt;

#define FREATURE_NUM 2	//特征数（输入向量维数）
#define LEARNING_RATE 1 //学习率 

typedef struct input_data&#123;
	double freature[FREATURE_NUM];
	int label;
&#125;input_data;
typedef struct input_dataset&#123;
	input_data* input;
	int set_num;
&#125;input_dataset;

double weight[FREATURE_NUM]=&#123;0&#125;;

void train(input_dataset dataset,int iteration);
void perceptron(input_data *input);

#endif


//perceptron.c
#include&quot;perceptron.h&quot;

void train(input_dataset dataset,int iteration)
&#123;
	//生成随机数种子 
	srand((unsigned)time(NULL));
	
	int set_num=dataset.set_num;
	int i,j,k;
	for(i=0;i&lt;iteration;i++)&#123;
		k=rand()%set_num;
		//梯度下降方法搜寻
		for(j=0;j&lt;FREATURE_NUM;j++)
		&#123;
			weight[j]+=1.0*LEARNING_RATE*dataset.input[k].freature[j]*dataset.input[k].label;
//			printf(&quot;%lf %lf\n&quot;,weight[j],dataset.input[k].freature[j]);
		&#125;
	&#125;
	return; 
&#125;

void perceptron(input_data *input)&#123;
	int i,temp;
	for(i=0,temp=0;i&lt;FREATURE_NUM;i++)temp+=weight[i]*input-&gt;freature[i];
	if(temp&gt;=0)input-&gt;label=1;
	else input-&gt;label=-1;
	
	printf(&quot;label:%d\n&quot;,input-&gt;label);
	return;
&#125;


#include&lt;stdio.h&gt;
#include&quot;perceptron.c&quot;

int main()&#123;
	
	input_data input[2];
	input[0].freature[0]=-3.0;
	input[0].freature[1]=1.0;
	input[0].label=1;
	input[1].freature[0]=-1.0;
	input[1].freature[1]=3.0;
	input[1].label=1;
	input[2].freature[0]=2.0;
	input[2].freature[1]=4;
	input[2].label=-1;
	input[3].freature[0]=4.0;
	input[3].freature[1]=-2.0;
	input[3].label=-1;
	
	input_dataset dataset;
	dataset.input=input;
	dataset.set_num=4;
	
	train(dataset,10);
	
	int i;
	for(i=0;i&lt;FREATURE_NUM;i++)printf(&quot;%lf\n&quot;,weight[i]);
	
	input_data test;
	scanf(&quot;%lf%lf&quot;,&amp;test.freature[0],&amp;test.freature[1]);
	perceptron(&amp;test);
	
	return 0;
&#125;
</code></pre>
<p> python实现及mnist手写数字识别（两类）：[NeuralNetworks&#x2F;03-Perceptron at<br>main](<a target="_blank" rel="noopener" href="https://github.com/microsoft/AI-For-">https://github.com/microsoft/AI-For-</a><br>Beginners&#x2F;tree&#x2F;main&#x2F;lessons&#x2F;3-NeuralNetworks&#x2F;03-Perceptron<br>“NeuralNetworks&#x2F;03-Perceptron at main”)</p>
<p> （特征：28pix*28pix）</p>
<p>实现N类感知器：训练N个感知器：</p>
<ol>
<li>Create 10 <strong><em>one-vs-all</em> datasets</strong> for all digits</li>
<li>Train <strong>10 perceptrons</strong></li>
<li>Define <code>classify</code> function to perform digit classification</li>
<li>Measure the accuracy of classification and print <em>confusion matrix</em></li>
<li>[Optional] Create improved <code>classify</code> function that performs the classification using one matrix multiplication.</li>
</ol>
<h2 id="五-Multi-Layered-Perceptron"><a href="#五-Multi-Layered-Perceptron" class="headerlink" title="五.Multi-Layered Perceptron"></a>五.Multi-Layered Perceptron</h2><p><strong>简介：</strong></p>
<p>we will extend themodel above into a more flexible framework, allowing us to:</p>
<ul>
<li>perform <strong>multi-class classification</strong>  in addition to two-class</li>
<li>solve <strong>regression problems</strong>  in addition to classification</li>
<li>separate classes that are not linearly separable</li>
</ul>
<p>We will also develop our own modular framework in Python that will allow us to<br>construct different neural network architectures.</p>
<p>Suppose we have a training dataset <strong>X</strong>  with labels <strong>Y</strong> , and we need to<br>build a **model  <em>f</em>  **that will make most accurate predictions. The quality<br>of predictions is measured by <strong>Loss function</strong>  <strong>ℒ</strong>. The following loss<br>functions are often used:</p>
<ul>
<li>For <strong>regression problem(回归问题)</strong> , when we need to predict a number, we can use <strong>absolute error</strong>    **∑i|f(x(i))-y(i)|   **, or <strong>squared error</strong>    **∑i(f(x(i))-y(i))^2   **</li>
<li>For <strong>classification(分类问题)</strong> , we use <strong>0-1 loss</strong>  (which is essentially the same as <strong>accuracy</strong>  of the model), or <strong>logistic loss</strong>.</li>
</ul>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/621e77f5fb5595bedf41e588670f4c80.png"></p>
<p>从p对损失函数L的影响来看逻辑损失函数更好</p>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/11de84ee5edc6ce0c8007f9604f71c73.png"></p>
<p>For <strong>one-level perceptron</strong> , function <em>f</em>  was defined as a **linear<br>function  <em>f(x)&#x3D;wx+b</em>  **(here <em>w</em>  is the weight matrix, <em>x</em>  is the vector<br>of input features, and <strong><em>b</em>  is bias vector</strong>). For different neural network<br>architectures, this function can take more <strong>complex form.</strong></p>
<blockquote>
<p>In the case of <strong>classification</strong> , it is often desirable to get<br><strong>probabilities</strong> of corresponding classes as <strong>network output.</strong>  To<br>convert arbitrary numbers to probabilities (eg. to normalize the output), we<br>often use <strong>softmax</strong>  function <strong>σ</strong> , and the function <em>f</em>  becomes<br><em>f(x)&#x3D;σ(wx+b)</em></p>
</blockquote>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/1760fe528375040727919cbb4587890d.png"></p>
<p>In the definition of <em>f</em>  above, <em>w</em>  and <em>b</em>  are called <strong>parameters</strong>  θ&#x3D;〈<br><em>w,b</em> 〉. Given the dataset 〈<strong>X</strong> ,<strong>Y</strong> 〉, we can compute an overall error on<br>the whole dataset as a function of parameters θ.</p>
<blockquote>
<p>✅ <strong>The goal of neural network training is to minimize the error （Loss<br>function</strong> <strong>ℒ</strong><strong>） by varying parameters θ</strong></p>
</blockquote>
<h3 id="1-Gradient-Descent-Optimization（梯度下降算法）"><a href="#1-Gradient-Descent-Optimization（梯度下降算法）" class="headerlink" title="1.Gradient Descent Optimization（梯度下降算法）"></a>1.Gradient Descent Optimization（梯度下降算法）</h3><p>This can be formalized as follows:</p>
<ul>
<li>Initialize parameters by some random values w(0), b(0)</li>
<li>Repeat the following step many times: <ul>
<li>w(i+1) &#x3D; w(i)-η∂ℒ&#x2F;∂w</li>
<li>b(i+1) &#x3D; b(i)-η∂ℒ&#x2F;∂b</li>
</ul>
</li>
</ul>
<p>During training, the optimization steps are supposed to be calculated<br>considering the whole dataset (remember that loss is calculated as a sum<br>through all training samples). However, in real life we take small portions of<br>the dataset called <strong>minibatches</strong> , and calculate gradients based on a subset<br>of data. Because subset is taken randomly each time, such method is called<br><strong>stochastic gradient descent</strong>  (SGD).</p>
<h3 id="2-Multi-Layered-Perceptrons-and-Backpropagation（多层感知器和反向传播）"><a href="#2-Multi-Layered-Perceptrons-and-Backpropagation（多层感知器和反向传播）" class="headerlink" title="2.Multi-Layered Perceptrons and Backpropagation（多层感知器和反向传播）"></a><strong>2.Multi-Layered Perceptrons and Backpropagation（多层感知器和反向传播）</strong></h3><p><strong>一个示例——两层感知器</strong> ：</p>
<p>One-layer network, as we have seen above, is capable of classifying linearly<br>separable classes. To build a richer model, we can <strong>combine several layers of<br>the network</strong>. Mathematically it would mean that the function <em>f</em>  would have<br>a more complex form, and will be computed in several steps:</p>
<ul>
<li>z1&#x3D;w1x+b1</li>
<li>z2&#x3D;w2α(z1)+b2</li>
<li>f &#x3D; σ(z2)</li>
</ul>
<p>Here, <strong>α</strong>  is a <strong>non-linear activation function</strong> , <strong>σ  is a softmax<br>function</strong>, and parameters θ&#x3D;&lt;_w1,b1,w2,b2_ &gt;.</p>
<p>The gradient descent algorithm would remain the same, but it would be more<br>difficult to calculate gradients. Given the chain differentiation rule, we can<br>calculate derivatives as:</p>
<ul>
<li>∂ℒ&#x2F;∂w2 &#x3D; (∂ℒ&#x2F;∂σ)(∂σ&#x2F;∂z2)(∂z2&#x2F;∂w2)</li>
<li>∂ℒ&#x2F;∂w1 &#x3D; (∂ℒ&#x2F;∂σ)(∂σ&#x2F;∂z2)(∂z2&#x2F;∂α)(∂α&#x2F;∂z1)(∂z1&#x2F;∂w1)</li>
</ul>
<blockquote>
<p>✅ The <strong>chain differentiation rule</strong> is used to calculate derivatives of the<br>loss function with respect to parameters.</p>
</blockquote>
<p><strong>链式规则、后向传播更新参数θ</strong> ：</p>
<p>Note that the left-most part of all those expressions is the same, and thus we<br>can** effectively calculate derivatives** **starting from the loss function<br>and going  “backwards” **through the computational graph. Thus the method of<br>training a multi-layered perceptron is called <strong>backpropagation</strong> , or<br>‘backprop’.</p>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/98446f8f75153030ff57d774b4684b02.png"></p>
<p>即：</p>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/82d7932c55daa1260a98b128ecb829ba.png"></p>
<h3 id="3-单层感知器模型"><a href="#3-单层感知器模型" class="headerlink" title="3.单层感知器模型"></a>3.单层感知器模型</h3><p><img src="https://i-blog.csdnimg.cn/blog_migrate/28b6864aabe5cda4e0786e846d3f43df.png"></p>
<pre><code>    Two outputs of the network correspond to two classes, and the class with highest value among two outputs corresponds to the right solution.
</code></pre>
<p>The model is defined as：</p>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/093e6e1eb8cb98c2846ce0e7fac93586.png"></p>
<p><strong>相关依赖：</strong></p>
<pre><code>import matplotlib.pyplot as plt 
from matplotlib import gridspec
from sklearn.datasets import make_classification
import numpy as np
# pick the seed for reproducibility - change it to explore the effects of random variations
np.random.seed(0)
import random
</code></pre>
<h4 id="1-创建数据集，X为特征向量，Y为标签："><a href="#1-创建数据集，X为特征向量，Y为标签：" class="headerlink" title="&lt; 1&gt;创建数据集，X为特征向量，Y为标签："></a><strong>&lt; 1&gt;创建数据集，X为特征向量，Y为标签：</strong></h4><pre><code>n = 100
X, Y = make_classification(n_samples = n, n_features=2,
                           n_redundant=0, n_informative=2, flip_y=0.2)
X = X.astype(np.float32)
Y = Y.astype(np.int32)

# Split into train and test dataset
train_x, test_x = np.split(X, [n*8//10])
train_labels, test_labels = np.split(Y, [n*8//10])


#显示数据集
print(train_x[:5])
print(train_labels[:5])


[[-0.836906  -1.382417 ]
 [ 3.0352616 -1.1195285]
 [ 1.6688806  2.4989042]
 [-0.5790065  2.1814067]
 [-0.8730455 -1.4692409]]
[0 1 1 1 0]
</code></pre>
<h4 id="2-前向传播计算过程："><a href="#2-前向传播计算过程：" class="headerlink" title="**&lt; 2&gt; 前向传播计算过程： **"></a>**&lt; 2&gt; 前向传播计算过程： **</h4><pre><code>class Linear:
    #初始化权重
    def __init__(self,nin,nout):
        self.W = np.random.normal(0, 1.0/np.sqrt(nin), (nout, nin))
        self.b = np.zeros((1,nout))
    #前向传播计算    
    def forward(self, x):
        return np.dot(x, self.W.T) + self.b
    
net = Linear(2,2)
net.forward(train_x[0:5])


#5个输入的输出
0,1.772021,-0.253845
1,0.283708,-0.396106
2,-0.300974,0.305132
3,-0.812048,0.560794
4,-1.235197,0.339497
</code></pre>
<h4 id="使用softmax函数转换为概率："><a href="#使用softmax函数转换为概率：" class="headerlink" title="&lt;3&gt;使用softmax函数转换为概率："></a>&lt;3&gt;使用softmax函数转换为概率：</h4><pre><code>class Softmax:
    def forward(self,z):
        zmax = z.max(axis=1,keepdims=True)
        expz = np.exp(z-zmax)
        Z = expz.sum(axis=1,keepdims=True)
        return expz / Z

softmax = Softmax()
softmax.forward(net.forward(train_x[0:10]))


        In case we have more than 2 classes, softmax will normalize probabilities across all of them.
</code></pre>
<h4 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="&lt;4&gt;交叉熵损失函数"></a>&lt;4&gt;交叉熵损失函数</h4><pre><code>        A loss function in classification is typically a **logistic function** , which can **be generalized as cross-entropy loss**. Cross-entropy loss is a function that can calculate similarity between two arbitrary probability distributions. 


def cross_ent(prediction, ground_truth):
    t = 1 if ground_truth &gt; 0.5 else 0
    return -t * np.log(prediction) - (1 - t) * np.log(1 - prediction)
plot_cross_ent()
</code></pre>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/2f4b2ad0f83c2804efe05f9563c4c85f.png"></p>
<pre><code>        Cross-entropy loss will be defined again as **a separate layer** , but `forward` function will have _**two input values: output of the previous layers of the network `p`, and the expected class `y`:**_
</code></pre>
<p><strong>应用：</strong></p>
<pre><code>class CrossEntropyLoss:
    def forward(self,p,y):
        self.p = p
        self.y = y
        p_of_y = p[np.arange(len(y)), y]
        log_prob = np.log(p_of_y)
        return -log_prob.mean() # average over all input samples

cross_ent_loss = CrossEntropyLoss()
p = softmax.forward(net.forward(train_x[0:10]))
cross_ent_loss.forward(p,train_labels[0:10])
</code></pre>
<blockquote>
<p><strong>IMPORTANT</strong> : Loss function returns a number that shows how good (or bad)<br>our network performs. It should return us one number for the whole dataset,<br>or for the part of the dataset (minibatch). Thus after calculating cross-<br>entropy loss for each individual component of the input vector, we need to<br>average (or add) all components together - which is done by the call to<br><code>.mean()</code>.</p>
<p>（注意计算的是<strong>交叉熵均值</strong> ：return -log_prob.mean() # average over all input samples ）</p>
</blockquote>
<pre><code>z = net.forward(train_x[0:10])    #输出
p = softmax.forward(z)            #softmax归一化
loss = cross_ent_loss.forward(p,train_labels[0:10])#cross_ent_loss = CrossEntropyLoss()
print(loss)
</code></pre>
<h4 id="Loss-Minimization-Problem-and-Network-Training："><a href="#Loss-Minimization-Problem-and-Network-Training：" class="headerlink" title="&lt;5&gt;Loss Minimization Problem and Network Training："></a>&lt;5&gt;Loss Minimization Problem and Network Training：</h4><p>数学描述：</p>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/44ff93f6ec92975a705328447d9fcb28.png"></p>
<p>采用梯度下降法进行计算（见2.）</p>
<p><strong>网络训练</strong> 包括前向和后向传播两个过程（<strong>原理</strong> 见2和3&lt;2&gt;）</p>
<p>One pass of the <strong>network training</strong> consists of two parts:</p>
<ul>
<li><strong>Forward pass</strong> , when we calculate the value of loss function for a given input minibatch</li>
<li><strong>Backward pass</strong> , when we try to minimize this error by distributing it back to the model parameters through the computational graph.</li>
</ul>
<p><strong>后向传播的具体实现：</strong></p>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/bcc3707448e65ae4040d2b305dc71a12.png"></p>
<p><em>注意参数的更新在一个minibatch完全计算完后，而不是单个样本</em></p>
<pre><code>def update(self,lr):
    self.W -= lr*self.dW    
    self.b -= lr*self.db
#LR是学习率
</code></pre>
<h4 id="函数小结"><a href="#函数小结" class="headerlink" title="&lt;6&gt;函数小结"></a>&lt;6&gt;函数小结</h4><pre><code>class Linear:
    def __init__(self,nin,nout):
        self.W = np.random.normal(0, 1.0/np.sqrt(nin), (nout, nin))
        self.b = np.zeros((1,nout))
        self.dW = np.zeros_like(self.W)
        self.db = np.zeros_like(self.b)
        
    def forward(self, x):
        self.x=x
        return np.dot(x, self.W.T) + self.b
    
    def backward(self, dz):
        dx = np.dot(dz, self.W)
        dW = np.dot(dz.T, self.x)
        db = dz.sum(axis=0)
        self.dW = dW
        self.db = db
        return dx
    
    def update(self,lr):
        self.W -= lr*self.dW
        self.b -= lr*self.db


class Softmax:
    def forward(self,z):
        self.z = z
        zmax = z.max(axis=1,keepdims=True)
        expz = np.exp(z-zmax)
        Z = expz.sum(axis=1,keepdims=True)
        return expz / Z
    def backward(self,dp):
        p = self.forward(self.z)
        pdp = p * dp
        return pdp - p * pdp.sum(axis=1, keepdims=True)
    
class CrossEntropyLoss:
    def forward(self,p,y):
        self.p = p
        self.y = y
        p_of_y = p[np.arange(len(y)), y]
        log_prob = np.log(p_of_y)
        return -log_prob.mean()
    def backward(self,loss):
        dlog_softmax = np.zeros_like(self.p)
        dlog_softmax[np.arange(len(self.y)), self.y] -= 1.0/len(self.y)
        return dlog_softmax / self.p
</code></pre>
<h4 id="Training-the-Model"><a href="#Training-the-Model" class="headerlink" title="&lt;7&gt;Training the Model"></a>&lt;7&gt;Training the Model</h4><pre><code>    Now we are ready to write the **training loop** , which will go through our dataset, and perform the optimization minibatch by minibatch._One complete pass through the dataset is often called**an epoch** :_


lin = Linear(2,2)
softmax = Softmax()
cross_ent_loss = CrossEntropyLoss()

learning_rate = 0.1

pred = np.argmax(lin.forward(train_x),axis=1)
acc = (pred==train_labels).mean()
print(&quot;Initial accuracy: &quot;,acc)

batch_size=4
for i in range(0,len(train_x),batch_size):
    xb = train_x[i:i+batch_size]
    yb = train_labels[i:i+batch_size]
    
    # forward pass
    z = lin.forward(xb)
    p = softmax.forward(z)
    loss = cross_ent_loss.forward(p,yb)
    
    # backward pass
    dp = cross_ent_loss.backward(loss)
    dz = softmax.backward(dp)
    dx = lin.backward(dz)
    lin.update(learning_rate)
    
pred = np.argmax(lin.forward(train_x),axis=1)
acc = (pred==train_labels).mean()
print(&quot;Final accuracy: &quot;,acc)
    


Initial accuracy:  0.2625
Final accuracy:  0.7875
</code></pre>
<h3 id="4-网络模型"><a href="#4-网络模型" class="headerlink" title="4.网络模型"></a>4.网络模型</h3><h4 id="定义网络类-："><a href="#定义网络类-：" class="headerlink" title="&lt;1&gt;定义网络类 ："></a>&lt;1&gt;定义<strong>网络类</strong> ：</h4><pre><code>    Since in many cases neural network is just **a composition of layers** , we can build a class that will allow us to **stack layers together** and**make forward and backward passes** through them without explicitly programming that logic. We will **store the list of layers inside the`Net` class**, and **use`add()` function to add new layers**:


class Net:
    def __init__(self):
        self.layers = []
    
    def add(self,l):
        self.layers.append(l)
        
    def forward(self,x):
        for l in self.layers:
            x = l.forward(x)
        return x
    
    def backward(self,z):
        for l in self.layers[::-1]:
            z = l.backward(z)
        return z
    
    def update(self,lr):
        for l in self.layers:
            if &#39;update&#39; in l.__dir__():
                l.update(lr)
</code></pre>
<p>定义网络和训练：</p>
<pre><code>net = Net()
net.add(Linear(2,2))
net.add(Softmax())
loss = CrossEntropyLoss()

def get_loss_acc(x,y,loss=CrossEntropyLoss()):
    p = net.forward(x)
    l = loss.forward(p,y)
    pred = np.argmax(p,axis=1)
    acc = (pred==y).mean()
    return l,acc

print(&quot;Initial loss=&#123;&#125;, accuracy=&#123;&#125;: &quot;.format(*get_loss_acc(train_x,train_labels)))

def train_epoch(net, train_x, train_labels, loss=CrossEntropyLoss(), batch_size=4, lr=0.1):
    for i in range(0,len(train_x),batch_size):
        xb = train_x[i:i+batch_size]
        yb = train_labels[i:i+batch_size]

        p = net.forward(xb)
        l = loss.forward(p,yb)
        dp = loss.backward(l)
        dx = net.backward(dp)
        net.update(lr)
 
train_epoch(net,train_x,train_labels)
        
print(&quot;Final loss=&#123;&#125;, accuracy=&#123;&#125;: &quot;.format(*get_loss_acc(train_x,train_labels)))
print(&quot;Test loss=&#123;&#125;, accuracy=&#123;&#125;: &quot;.format(*get_loss_acc(test_x,test_labels)))


Initial loss=0.8977914474068779, accuracy=0.4625: 
Final loss=0.47908832233966514, accuracy=0.825: 
Test loss=0.5317198099647931, accuracy=0.8:
</code></pre>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/9f0836ab887616dbe48bcedfff0a5ea8.png"></p>
<h4 id="Multi-Layered-Models"><a href="#Multi-Layered-Models" class="headerlink" title="&lt;2&gt;Multi-Layered Models"></a>&lt;2&gt;Multi-Layered Models</h4><pre><code>        Very important thing to note, however, is that _**in between linear layers we need to have a non-linear** **activation function**_ , such as _**tanh**_. Without such non-linearity, several linear layers would have the same expressive power as just one layers - because _**composition of linear functions is also linear!**_
</code></pre>
<p>在线性层之间添加激活函数，线性函数的叠加仍是线性。</p>
<pre><code>class Tanh:
    def forward(self,x):
        y = np.tanh(x)
        self.y = y
        return y
    def backward(self,dy):
        return (1.0-self.y**2)*dy

    Adding several layers make sense, because unlike one-layer network, multi-layered model will **be able to accuratley classify sets that are not linearly separable**. I.e., a model with several layers will be **reacher**.
</code></pre>
<blockquote>
<p>It can be demonstrated that with sufficient number of neurons a <strong>two-<br>layered model</strong> is capable to classifying any <strong>convex set of data points</strong><br>, and <strong>three-layered network</strong> can classify <strong>virtually any set.</strong></p>
</blockquote>
<p>多层网络的形式见前（2.）</p>
<p>两层网络示例：</p>
<pre><code>net = Net()
net.add(Linear(2,10))
net.add(Tanh())
net.add(Linear(10,2))
net.add(Softmax())
loss = CrossEntropyLoss()
</code></pre>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/aa5a7f818f593a302d689193dc8bf872.png"></p>
<p>关于线性模型和多层复杂模型的区别和**过拟合（**<strong>overfitting）</strong> 问题：</p>
<p><strong>A linear model:</strong></p>
<ul>
<li>We are likely to get high training loss - so-called <strong>underfitting</strong> , when the model does not have enough power to correctly separate all data.</li>
<li>Valiadation loss and training loss are more or less the same. The model is likely to generalize well to test data.</li>
</ul>
<p><strong>Complex multi-layered model</strong></p>
<ul>
<li>Low training loss - the model can approximate training data well, because it has enough expressive power.</li>
<li>Validation loss can be much higher than training loss and can start to increase during training - this is because the model “memorizes” training points, and loses the “overall picture”</li>
</ul>
<p><strong>小结：</strong></p>
<p><strong>Takeaways</strong></p>
<ul>
<li>Simple models (fewer layers, fewer neurons) with low number of parameters (“low capacity”) are less likely to overfit</li>
<li>More complex models (more layers, more neurons on each layer, high capacity) are likely to overfit. We need to monitor validation error to make sure it does not start to rise with further training</li>
<li>More complex models need more data to train on.</li>
<li>You can solve overfitting problem by either: <ul>
<li>simplifying your model</li>
<li>increasing the amount of training data</li>
</ul>
</li>
<li><strong>Bias-variance trade-off</strong> is a term that shows that you need to get the compromise <ul>
<li>between power of the model and amount of data,</li>
<li>between overfittig and underfitting</li>
</ul>
</li>
<li>There is not single recipe on how many layers of parameters you need - the best way is to experiment</li>
</ul>
<h3 id="5-代码整合"><a href="#5-代码整合" class="headerlink" title="5.代码整合"></a>5.代码整合</h3><pre><code>###################################################################
# package
# matplotlib nbagg
import matplotlib.pyplot as plt 
from matplotlib import gridspec
from sklearn.datasets import make_classification
import numpy as np
# pick the seed for reproducibility - change it to explore the effects of random variations
np.random.seed(0)
import random


###################################################################
# dataset
n = 100
X, Y = make_classification(n_samples = n, n_features=2,
                           n_redundant=0, n_informative=2, flip_y=0.2)
X = X.astype(np.float32)
Y = Y.astype(np.int32)

# Split into train and test dataset
train_x, test_x = np.split(X, [n*8//10])
train_labels, test_labels = np.split(Y, [n*8//10])


###################################################################
# layers
class Linear:
    def __init__(self,nin,nout):
        self.W = np.random.normal(0, 1.0/np.sqrt(nin), (nout, nin))
        self.b = np.zeros((1,nout))
        self.dW = np.zeros_like(self.W)
        self.db = np.zeros_like(self.b)
        
    def forward(self, x):
        self.x=x
        return np.dot(x, self.W.T) + self.b
    
    def backward(self, dz):
        dx = np.dot(dz, self.W)
        dW = np.dot(dz.T, self.x)
        db = dz.sum(axis=0)
        self.dW = dW
        self.db = db
        return dx
    
    def update(self,lr):
        self.W -= lr*self.dW
        self.b -= lr*self.db

class Tanh:
    def forward(self,x):
        y = np.tanh(x)
        self.y = y
        return y
    def backward(self,dy):
        return (1.0-self.y**2)*dy


class Softmax:
    def forward(self,z):
        self.z = z
        zmax = z.max(axis=1,keepdims=True)
        expz = np.exp(z-zmax)
        Z = expz.sum(axis=1,keepdims=True)
        return expz / Z
    def backward(self,dp):
        p = self.forward(self.z)
        pdp = p * dp
        return pdp - p * pdp.sum(axis=1, keepdims=True)
    

class CrossEntropyLoss:
    def forward(self,p,y):
        self.p = p
        self.y = y
        p_of_y = p[np.arange(len(y)), y]
        log_prob = np.log(p_of_y)
        return -log_prob.mean()
    def backward(self,loss):
        dlog_softmax = np.zeros_like(self.p)
        dlog_softmax[np.arange(len(self.y)), self.y] -= 1.0/len(self.y)
        return dlog_softmax / self.p


###################################################################
# network
class Net:
    def __init__(self):
        self.layers = []
    
    def add(self,l):
        self.layers.append(l)
        
    def forward(self,x):
        for l in self.layers:
            x = l.forward(x)
        return x
    
    def backward(self,z):
        for l in self.layers[::-1]:
            z = l.backward(z)
        return z
    
    def update(self,lr):
        for l in self.layers:
            if &#39;update&#39; in l.__dir__():
                l.update(lr)

def get_loss_acc(x,y,loss=CrossEntropyLoss()):
    p = net.forward(x)
    l = loss.forward(p,y)
    pred = np.argmax(p,axis=1)
    acc = (pred==y).mean()
    return l,acc

def train_epoch(net, train_x, train_labels, loss=CrossEntropyLoss(), batch_size=4, lr=0.1):
    for i in range(0,len(train_x),batch_size):
        xb = train_x[i:i+batch_size]
        yb = train_labels[i:i+batch_size]

        p = net.forward(xb)
        l = loss.forward(p,yb)
        dp = loss.backward(l)
        dx = net.backward(dp)
        net.update(lr)
        print(&quot;epoch=&#123;&#125;: &quot;.format(i),end=&quot;&quot;)
        print(&quot;Final loss=&#123;&#125;, accuracy=&#123;&#125;: &quot;.format(*get_loss_acc(train_x,train_labels)))
        print(&quot;Test loss=&#123;&#125;, accuracy=&#123;&#125;: &quot;.format(*get_loss_acc(test_x,test_labels)))

###################################################################
# main
net = Net()
net.add(Linear(2,10))
net.add(Tanh())
net.add(Linear(10,2))
net.add(Softmax())
train_epoch(net,train_x,train_labels)
</code></pre>
<h3 id="6-3-layer-network-实现-mnist-手写数字识别"><a href="#6-3-layer-network-实现-mnist-手写数字识别" class="headerlink" title="6.3-layer network 实现 mnist 手写数字识别"></a>6.3-layer network 实现 mnist 手写数字识别</h3><p>训练模型，保存结果：</p>
<pre><code>###################################################################
# packages
import matplotlib.pyplot as plt 
from matplotlib import gridspec
from sklearn.datasets import make_classification
import numpy as np
# pick the seed for reproducibility - change it to explore the effects of random variations
np.random.seed(0)
import random


###################################################################
# dataset
n=70000
# generate data
# X, Y = make_classification(n_samples = n, n_features=28*28,n_redundant=0, n_informative=8*8, flip_y=0.2)
# get data from mnist
from torchvision import datasets, transforms
mnist_train = datasets.MNIST(root=&#39;./data&#39;, train=True, transform=transforms.ToTensor())
X = mnist_train.data.numpy()
Y = mnist_train.targets.numpy()
X = X.reshape(X.shape[0],-1)
X = X.astype(np.float32)                            
Y = Y.astype(np.int32)

# Split into train and test dataset
train_x, test_x = np.split(X, [n*8//10])            # 80% training and 20% test
train_labels, test_labels = np.split(Y, [n*8//10])  


###################################################################
# layers
class Linear:
    def __init__(self,nin,nout):
        self.W = np.random.normal(0, 1.0/np.sqrt(nin), (nout, nin))
        self.b = np.zeros((1,nout))
        self.dW = np.zeros_like(self.W)
        self.db = np.zeros_like(self.b)
        
    def forward(self, x):
        self.x=x
        return np.dot(x, self.W.T) + self.b
    
    def backward(self, dz):
        dx = np.dot(dz, self.W)
        dW = np.dot(dz.T, self.x)
        db = dz.sum(axis=0)
        self.dW = dW
        self.db = db
        return dx
    
    def update(self,lr):
        self.W -= lr*self.dW
        self.b -= lr*self.db

class Tanh:
    def forward(self,x):
        y = np.tanh(x)
        self.y = y
        return y
    def backward(self,dy):
        return (1.0-self.y**2)*dy


class Softmax:
    def forward(self,z):
        self.z = z
        zmax = z.max(axis=1,keepdims=True)
        expz = np.exp(z-zmax)
        Z = expz.sum(axis=1,keepdims=True)
        return expz / Z
    def backward(self,dp):
        p = self.forward(self.z)
        pdp = p * dp
        return pdp - p * pdp.sum(axis=1, keepdims=True)
    

class CrossEntropyLoss:
    def forward(self,p,y):
        self.p = p
        self.y = y
        p_of_y = p[np.arange(len(y)), y]
        log_prob = np.log(p_of_y)
        return -log_prob.mean()
    def backward(self,loss):
        dlog_softmax = np.zeros_like(self.p)
        dlog_softmax[np.arange(len(self.y)), self.y] -= 1.0/len(self.y)
        return dlog_softmax / self.p


###################################################################
# network
class Net:
    def __init__(self):
        self.layers = []
    
    def add(self,l):
        self.layers.append(l)
        
    def forward(self,x):
        for l in self.layers:
            x = l.forward(x)
        return x
    
    def backward(self,z):
        for l in self.layers[::-1]:
            z = l.backward(z)
        return z
    
    def update(self,lr):
        for l in self.layers:
            if &#39;update&#39; in l.__dir__():
                l.update(lr)

def get_loss_acc(x,y,loss=CrossEntropyLoss()):
    p = net.forward(x)
    l = loss.forward(p,y)
    pred = np.argmax(p,axis=1)
    acc = (pred==y).mean()
    return l,acc

def train_epoch(net, train_x, train_labels, loss=CrossEntropyLoss(), batch_size=4, lr=0.1):
    for i in range(0,len(train_x),batch_size):
        xb = train_x[i:i+batch_size]
        yb = train_labels[i:i+batch_size]

        p = net.forward(xb)
        l = loss.forward(p,yb)
        dp = loss.backward(l)
        dx = net.backward(dp)
        net.update(lr)
        print(&quot;epoch=&#123;&#125;: &quot;.format(i//batch_size))
        print(&quot;Final loss=&#123;&#125;, accuracy=&#123;&#125;: &quot;.format(*get_loss_acc(train_x,train_labels)))
        print(&quot;Test loss=&#123;&#125;, accuracy=&#123;&#125;: &quot;.format(*get_loss_acc(test_x,test_labels)))

###################################################################
# main
if __name__ == &#39;__main__&#39;:
    # model
    net = Net()
    net.add(Linear(28*28,300))
    net.add(Tanh())
    net.add(Linear(300,10))
    net.add(Softmax())
    train_epoch(net,train_x,train_labels,batch_size=1000) 

    #save the model
    import pickle
    with open(&#39;model.pkl&#39;, &#39;wb&#39;) as f:
        pickle.dump(net, f)    
</code></pre>
<p>加载模型，进行测试：</p>
<pre><code>import OwnFramework
import torchvision
import numpy as np
import pickle
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import random

# import the model   
with open(&#39;model.pkl&#39;, &#39;rb&#39;) as f:
    OwnFramework.net = pickle.load(f)

# test the data from minst
test_data = torchvision.datasets.MNIST(&#39;./data&#39;, train=False, download=False)
test_x = test_data.data.numpy().reshape(-1,28*28)
test_labels = test_data.targets.numpy()

# test the model
print(&quot;Test loss=&#123;&#125;, accuracy=&#123;&#125;: &quot;.format(*OwnFramework.get_loss_acc(test_x,test_labels)))

# show the images and the predictions
fig=plt.figure(figsize=(8, 8))
gs = gridspec.GridSpec(4, 4)
for i in range(16):
    j=random.randint(0,len(test_x))
    ax = plt.subplot(gs[i])
    ax.imshow(test_x[j].reshape(28,28))
    ax.set_title(&quot;Predicted: &#123;&#125;&quot;.format(np.argmax(OwnFramework.net.forward(test_x[j:j+1]))))
    ax.axis(&#39;off&#39;)
plt.show()

# show the images that are not predicted not correctly
fig=plt.figure(figsize=(12, 8))
gs = gridspec.GridSpec(4, 4)
i=0
for j in range(len(test_x)):
    if np.argmax(OwnFramework.net.forward(test_x[j:j+1])) != test_labels[j]:
        ax = plt.subplot(gs[i])
        ax.imshow(test_x[j].reshape(28,28))
        ax.set_title(&quot;Predicted: &#123;&#125;, True: &#123;&#125;&quot;.format(np.argmax(OwnFramework.net.forward(test_x[j:j+1])),test_labels[j]))
        ax.axis(&#39;off&#39;)
        i+=1
    if i==16:
        break
plt.show()
</code></pre>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/0f2a5124db743b6101b0841143d3f591.png"></p>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/94e98f86a86028bb9b2d14b0ed7f50d3.png"></p>
<h2 id="六-Neural-Network-Frameworks"><a href="#六-Neural-Network-Frameworks" class="headerlink" title="六.Neural Network Frameworks"></a>六.Neural Network Frameworks</h2><p><strong>架构API：</strong></p>
<p>to be able to train neural networks efficiently we need to do two things:</p>
<ul>
<li><p>To <strong>operate</strong> on <strong>tensors</strong> , eg. to multiply, add, and compute some functions such as sigmoid or softmax</p>
</li>
<li><p>To compute <strong>gradients</strong> of all expressions, in order to perform gradient descent optimization</p>
<pre><code>While the **`numpy`  library** can **do the first part** , we need some mechanism to compute gradients. In our framework that we have developed in the previous section we had to manually program all derivative functions inside the `backward` method, which does backpropagation. Ideally, _**a framework should give us the opportunity to compute gradients of _any expression_  that we can define**_.

Another important thing is to be able to **perform computations on GPU** , or any other specialized compute units, such as [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit &quot;TPU&quot;). Deep neural network training requires _a lot_  of computations, and to be able to parallelize those computations on GPUs is very important.
</code></pre>
</li>
</ul>
<p><strong>底层和高层API：</strong>  </p>
<pre><code>    Currently, the two **most popular neural frameworks** are:** [TensorFlow](http://tensorflow.org/ &quot;TensorFlow&quot;) and [PyTorch](https://pytorch.org/ &quot;PyTorch&quot;).** Both provide a **low-level API** to operate with **tensors on both CPU and GPU**. On top of the low-level API, there is also **higher-level API** , called** [Keras](https://keras.io/ &quot;Keras&quot;) and [PyTorch Lightning](https://pytorchlightning.ai/ &quot;PyTorch Lightning&quot;) **correspondingly.
</code></pre>
<p>Low-Level API| <a target="_blank" rel="noopener" href="http://tensorflow.org/" title="TensorFlow">TensorFlow</a>|</p>
<p><a target="_blank" rel="noopener" href="https://pytorch.org/" title="PyTorch">PyTorch</a><br>—|—|—<br>High-level API| <a target="_blank" rel="noopener" href="https://keras.io/" title="Keras">Keras</a>| <a target="_blank" rel="noopener" href="https://pytorchlightning.ai/" title="PyTorch Lightning">PyTorch<br>Lightning</a>  </p>
<p><strong>Low-level APIs</strong>  in both frameworks allow you to build so-called<br><strong>computational graphs</strong>. This graph defines how to compute the output<br>(usually <strong>the loss function</strong>) <strong>with given input parameters</strong> , and can be<br>pushed for computation <strong>on GPU</strong> , if it is available. There are functions to<br>differentiate this computational graph and compute gradients, which can then<br>be used for optimizing model parameters.</p>
<p><strong>High-level APIs</strong>  pretty much consider neural networks as a <strong>sequence of<br>layers</strong> , and make constructing most of the neural networks much easier.<br>Training the model usually requires <strong>preparing the data</strong> and then calling a<br><strong><code>fit</code> function</strong> to do the job.</p>
<pre><code>    The high-level API allows you to construct typical neural networks **very quickly without worrying about lots of details**. At the same time, low-level API offer much more control over the training process, and thus they are **used a lot in research** , when you are dealing with **new neural network architectures.**

    It is also important to understand that you can**use both APIs together** , eg. you can develop your own network layer architecture using low-level API, and then use it inside the larger network constructed and trained with the high-level API. Or you can define a network using the high-level API as a sequence of layers, and then use your own low-level training loop to perform optimization. Both APIs use the same basic underlying concepts, and they are designed to work well together.
</code></pre>
<p><strong>过拟合检测：</strong></p>
<p><strong>How to detect overfitting</strong></p>
<pre><code>    As you can see from the graph above, overfitting can be detected by a very low training error, and a high validation error. Normally during training we will see both training and validation errors starting to decrease, and then **at some point validation error might stop decreasing and start rising**. This will be a sign of overfitting, and the indicator that we should probably **stop training at this point**  (or at least **make a snapshot of the model**).（及时备份）
</code></pre>
<p><img src="https://i-blog.csdnimg.cn/blog_migrate/9c3db5d1878b3ecef22e81631f39e2b0.png"></p>
<h3 id="1-Keras"><a href="#1-Keras" class="headerlink" title="1.Keras"></a>1.Keras</h3><pre><code>    Keras is **a part of Tensorflow 2.x framework**. Let’s make sure we have version 2.x.x of Tensorflow installed:


# packages
import tensorflow as tf
from tensorflow import keras
import numpy as np
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
print(f&#39;Tensorflow version = &#123;tf.__version__&#125;&#39;)


# data prepare
np.random.seed(0) # pick the seed for reproducibility - change it to explore the effects of random variations

n = 100
X, Y = make_classification(n_samples = n, n_features=2,
                           n_redundant=0, n_informative=2, flip_y=0.05,class_sep=1.5)
X = X.astype(np.float32)
Y = Y.astype(np.int32)

split = [ 70*n//100 ]
train_x, test_x = np.split(X, split)
train_labels, test_labels = np.split(Y, split)
</code></pre>
<p>**关于张量的概念：（多维向量）  **</p>
<p><strong>Tensor</strong> is a <strong>multi-dimensional array</strong>. It is very convenient to use<br>tensors to represent different types of data:</p>
<ul>
<li>400x400 - black-and-white picture</li>
<li>400x400x3 - color picture</li>
<li>16x400x400x3 - minibatch of 16 color pictures</li>
<li>25x400x400x3 - one second of 25-fps video</li>
<li>8x25x400x400x3 - minibatch of 8 1-second videos</li>
</ul>
<p>Tensors give us a convenient way to represent input&#x2F;output data, as well we<br>weights inside the neural network.</p>
<p><strong>归一化数据：（约束网络参数范围）Normalizing Data</strong></p>
<p>Before training, it is common to bring our input features<strong>to the standard<br>range of [0,1] (or [-1,1]).</strong> The exact reasons for that we will discuss later<br>in the course, but in short the reason is the following. We want to avoid<br>values that flow through our network <strong>getting too big or too small</strong> , and we<br>normally agree to keep all values in the small range close to 0. Thus we<br><strong>initialize the weights with small random numbers</strong> , and we keep signals in<br>the same range.</p>
<pre><code>train_x_norm = (train_x-np.min(train_x,axis=0)) / (np.max(train_x,axis=0)-np.min(train_x,axis=0))
test_x_norm = (test_x-np.min(train_x,axis=0)) / (np.max(train_x,axis=0)-np.min(train_x,axis=0))
</code></pre>
<h4 id="Training-One-Layer-Network-Perceptron"><a href="#Training-One-Layer-Network-Perceptron" class="headerlink" title="&lt;1&gt;Training One-Layer Network (Perceptron)"></a>&lt;1&gt;Training One-Layer Network (Perceptron)</h4><h5 id="①模型定义"><a href="#①模型定义" class="headerlink" title="①模型定义"></a>①模型定义</h5><p>In many cases, a neural network would be <strong>a sequence of layers.</strong> It can be<br>defined in Keras using **<code>Sequential</code> **model in the following manner:</p>
<pre><code>model = keras.models.Sequential()
model.add(keras.Input(shape=(2,)))
model.add(keras.layers.Dense(1))
model.add(keras.layers.Activation(keras.activations.sigmoid))
model.summary()

# or
# Input size, as well as activation function, can also be specified directly in the Dense layer for brevity:
model = keras.models.Sequential()
model.add(keras.layers.Dense(1,input_shape=(2,),activation=&#39;sigmoid&#39;))
model.summary()
</code></pre>
<p>说明：</p>
<p>Here, we first create the model, and then add layers to it:</p>
<ul>
<li><p>First <code>Input</code> layer (<strong>which is not strictly speaking a layer</strong>) contains the specification of network’s <strong>input size</strong></p>
</li>
<li><p><code>Dense</code> layer is the actual perceptron that <strong>contains trainable weights</strong></p>
</li>
<li><p>Finally, there is a layer with **<em>sigmoid</em> <code>Activation</code> function **to bring the result of the network into 0-1 range (to make it a probability).</p>
<h1 id="Model-“sequential”-Layer-type-Output-Shape-Param"><a href="#Model-“sequential”-Layer-type-Output-Shape-Param" class="headerlink" title="Model: “sequential”_________________________________________________________________ Layer (type)                Output Shape              Param #"></a>Model: “sequential”<br>_________________________________________________________________<br> Layer (type)                Output Shape              Param #</h1><p> dense (Dense)               (None, 1)                 3</p>
<p> activation (Activation)     (None, 1)                 0</p>
<p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;<br>Total params: 3 (12.00 Byte)<br>Trainable params: 3 (12.00 Byte)<br>Non-trainable params: 0 (0.00 Byte)</p>
<hr>
</li>
</ul>
<h5 id="②模型编译（指定损失函数、优化方法【梯度下降等】、精度）"><a href="#②模型编译（指定损失函数、优化方法【梯度下降等】、精度）" class="headerlink" title="②模型编译（指定损失函数、优化方法【梯度下降等】、精度）"></a>②模型编译（指定损失函数、优化方法【梯度下降等】、精度）</h5><p>Before training the model, we need to <strong>compile it</strong> , which essentially mean<br>specifying:</p>
<ul>
<li><strong>Loss function</strong> , which defines how loss is calculated. Because we have two-class classification problem, we will use <em>binary cross-entropy loss</em>.</li>
<li><strong>Optimizer</strong> to use. The simplest option would be to use <code>sgd</code> for <em>stochastic gradient descent</em> , or you can use more sophisticated optimizers such as <code>adam</code>.</li>
<li><strong>Metrics</strong> that we want to use to measure success of our training. Since it is classification task, a good metrics would be <code>Accuracy</code> (or <code>acc</code> for short)</li>
</ul>
<p>We can specify loss, metrics and optimizer either as <strong>strings</strong> , or by<br>providing some <strong>objects from Keras framework</strong>. In our example, we need to<br>**specify<code>learning_rate</code> parameter **to fine-tune learning speed of our model,<br>and thus we provide <strong>full name of Keras SGD optimizer.</strong></p>
<p><strong>（可使用字符串或对象来定义）</strong></p>
<pre><code>model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.2),loss=&#39;binary_crossentropy&#39;,metrics=[&#39;acc&#39;])
</code></pre>
<h5 id="③训练"><a href="#③训练" class="headerlink" title="③训练"></a>③训练</h5><p>After compiling the model, we can do the actual training by calling <strong><code>fit</code><br>method.</strong> The most important parameters are:</p>
<ul>
<li><p><code>x</code> and <code>y</code> specify <strong>training data, features and labels respectively</strong></p>
</li>
<li><p>If we want validation to be performed on each epoch, we can specify **<code>validation_data</code> **parameter, which would be a tuple of features and labels</p>
</li>
<li><p>**<code>epochs</code> **specified the number of epochs</p>
</li>
<li><p>If we want training to happen in <strong>minibatches</strong> , we can specify **<code>batch_size</code> **parameter. You can also pre-batch the data manually before passing it to <code>x</code>&#x2F;<code>y</code>&#x2F;<code>validation_data</code>, in which case you do not need <strong><code>batch_size</code></strong></p>
<p>model.fit(x&#x3D;train_x_norm,y&#x3D;train_labels,validation_data&#x3D;(test_x_norm,test_labels),epochs&#x3D;10,batch_size&#x3D;1)</p>
</li>
</ul>
<blockquote>
<p>Note that you can c<strong>all<code>fit</code> function several times</strong> in a row to <strong>further<br>train the network</strong>. If you want to <strong>start training from scratch</strong> - you<br>need to <strong>re-run the cell with the model definition.</strong></p>
</blockquote>
<p>注：训练是叠加的，想从头训练需<strong>重定义网络</strong></p>
<h4 id="Multi-Class-Classificatio（多分类问题）"><a href="#Multi-Class-Classificatio（多分类问题）" class="headerlink" title="&lt;2&gt;Multi-Class Classificatio（多分类问题）"></a>&lt;2&gt;Multi-Class Classificatio（多分类问题）</h4><pre><code>    If you need to solve a problem of multi-class classification, your network would have more that one output - corresponding to the number of classes .**Each output will contain the probability of a given class.（多类多输出）**
</code></pre>
<p>**         <strong>When you expect a network to output a set of probabilities , we<br>need all of them to add up to 1. To ensure this, we use <code>softmax</code> as a final<br>activation function on the last layer. <strong>Softmax</strong> takes a vector input, and<br>makes sure that all components of that vector are transformed into<br>probabilities.</strong>（softmax 使所有概率和为1）**</p>
<pre><code>    Also, since the output of the network is a C-dimensional vector, we need labels to have the same form. This can be achieved by using **one-hot encoding** , when the number of a class is i converted to **a vector of zeroes, with 1 at the i-th position.（独热码，一位1其他位0）**

    To compare the probability output of the neural network with expected one-hot-encoded label, we use **cross-entropy loss** function. It takes two probability distributions, and outputs a value of how different they are.**（概率输出和独热码标签计算交叉熵损失函数）**
</code></pre>
<p>So, to <strong>summarize</strong> what we need to do for multi-class classification with<br>classes:</p>
<ul>
<li><p>The network should have neurons in the last layer</p>
</li>
<li><p><strong>Last activation function</strong> should be <strong>softmax</strong></p>
</li>
<li><p>Loss should be <strong>cross-entropy loss</strong></p>
</li>
<li><p>Labels should be converted to <strong>one-hot encoding</strong> (this can be done using <code>numpy</code>, or using Keras utils <code>to_categorical</code>)</p>
<p>model &#x3D; keras.models.Sequential([<br>keras.layers.Dense(5,input_shape&#x3D;(2,),activation&#x3D;’relu’),<br>keras.layers.Dense(2,activation&#x3D;’softmax’)<br>])<br>model.compile(keras.optimizers.Adam(0.01),’categorical_crossentropy’,[‘acc’])</p>
<h1 id="Two-ways-to-convert-to-one-hot-encoding"><a href="#Two-ways-to-convert-to-one-hot-encoding" class="headerlink" title="Two ways to convert to one-hot encoding"></a>Two ways to convert to one-hot encoding</h1><p>train_labels_onehot &#x3D; keras.utils.to_categorical(train_labels)<br>test_labels_onehot &#x3D; np.eye(2)[test_labels]</p>
<p>hist &#x3D; model.fit(x&#x3D;train_x_norm,y&#x3D;train_labels_onehot,validation_data&#x3D;[test_x_norm,test_labels_onehot],batch_size&#x3D;1,epochs&#x3D;10)</p>
</li>
</ul>
<p><strong>Sparse Categorical Cross-Entropy（稀疏分类交叉熵）（使用整数标签代替独热码标签）</strong></p>
<p>Often labels in multi-class classification are represented by class numbers.<br>Keras also supports another kind of loss function called <strong>sparse categorical<br>crossentropy</strong> , which expects class number to be <strong>integers</strong> , and not one-<br>hot vectors. Using this kind of loss function, we can simplify our training<br>code:</p>
<pre><code>model.compile(keras.optimizers.Adam(0.01),&#39;sparse_categorical_crossentropy&#39;,[&#39;acc&#39;])
model.fit(x=train_x_norm,y=train_labels,validation_data=[test_x_norm,test_labels],batch_size=1,epochs=10)
</code></pre>
<h4 id="Multi-Label-Classification（多标签分类）"><a href="#Multi-Label-Classification（多标签分类）" class="headerlink" title="&lt;3&gt;Multi-Label Classification（多标签分类）"></a>&lt;3&gt;Multi-Label Classification（多标签分类）</h4><pre><code>    With multi-label classification, instead of one-hot encoded vector, we will **have a vector that has 1 in position corresponding to all classes** relevant to the input sample. Thus, output of the network should not have normalized probabilities for all classes, but rather for each class individually - which corresponds to using **sigmoid** activation function. Cross-entropy loss can still be used as a loss function.**（不再使用独热码，而是标签中所有包含的位为1）**
</code></pre>
<h4 id="总结-Summary-of-Classification-Loss-Functions"><a href="#总结-Summary-of-Classification-Loss-Functions" class="headerlink" title="&lt;4&gt;总结 Summary of Classification Loss Functions"></a>&lt;4&gt;总结 <strong>Summary of Classification Loss Functions</strong></h4><pre><code>    We have seen that binary, multi-class and multi-label classification **differ by the type of loss function and activation function on the last layer** of the network. It may all be a little bit confusing if you are just starting to learn, but here are a few rules to keep in mind:
</code></pre>
<ul>
<li>If the network has one output (<strong>binary classification</strong>), we use <strong>sigmoid</strong> <strong>activation function</strong> , for <strong>multiclass classification</strong> - <strong>softmax</strong></li>
<li>If the output class is represented as one-hot-encoding, the <strong>loss function</strong> will be <strong>cross entropy loss</strong> (categorical cross-entropy), if the output contains class number - <strong>sparse categorical cross-entropy</strong>. For <strong>binary classification</strong> - use <strong>binary cross-entropy</strong> (same as <strong>log loss</strong>)</li>
<li><strong>Multi-label classification</strong> is when we can have an object belonging to several classes at the same time. In this case, we need to encode labels using one-hot encoding, and use <strong>sigmoid</strong> as activation function, so that each class probability is between 0 and 1.</li>
</ul>
<table>
<thead>
<tr>
<th>Classification</th>
<th>Label Format</th>
<th>Activation Function</th>
<th>Loss</th>
</tr>
</thead>
<tbody><tr>
<td>Binary</td>
<td>Probability of 1st class</td>
<td>sigmoid</td>
<td>binary crossentropy</td>
</tr>
<tr>
<td>Binary</td>
<td>One-hot encoding (2 outputs)</td>
<td>softmax</td>
<td>categorical crossentropy</td>
</tr>
<tr>
<td>Multiclass</td>
<td>One-hot encoding</td>
<td>softmax</td>
<td>categorical crossentropy</td>
</tr>
<tr>
<td>Multiclass</td>
<td>Class Number</td>
<td>softmax</td>
<td>sparse categorical crossentropy</td>
</tr>
<tr>
<td>Multilabel</td>
<td>One-hot encoding</td>
<td>sigmoid</td>
<td>categorical crossentropy</td>
</tr>
</tbody></table>
<h3 id="2-Tensorflow2-x-Keras"><a href="#2-Tensorflow2-x-Keras" class="headerlink" title="2.Tensorflow2.x+Keras"></a>2.Tensorflow2.x+Keras</h3><pre><code>        Tensorflow 2.x + Keras - new version of Tensorflow with integrated Keras functionality, which supports **dynamic computation graph** , allowing to perform tensor operations very similar to numpy (and PyTorch)


import tensorflow as tf
import numpy as np
print(tf.__version__)
</code></pre>
<h4 id="简单张量操作"><a href="#简单张量操作" class="headerlink" title="&lt;1&gt;简单张量操作"></a>&lt;1&gt;简单张量操作</h4><h5 id="①创建"><a href="#①创建" class="headerlink" title="①创建"></a>①创建</h5><p>You can easily create simple tensors from lists of np-arrays, or generate<br>random ones</p>
<pre><code># 创建常量张量
a = tf.constant([[1,2],[3,4]])
print(a)
# 创建正态分布随机10*3张量
a = tf.random.normal(shape=(10,3))
print(a)
</code></pre>
<h5 id="②运算"><a href="#②运算" class="headerlink" title="②运算"></a>②运算</h5><p>You can use arithmetic operations on tensors, which are performed element-<br>wise, as in numpy. Tensors are automatically expanded to required dimension,<br>if needed. <strong>To extract numpy-array from tensor, use<br><code>.numpy()</code>:（将张量转化为np数组）（以下是运算示例：）</strong></p>
<pre><code>print(a-a[0])
print(tf.exp(a)[0].numpy())
</code></pre>
<h4 id="计算梯度"><a href="#计算梯度" class="headerlink" title="&lt;2&gt;计算梯度"></a>&lt;2&gt;计算梯度</h4><p>For back propagation, you need to compute gradients. This is done using<br><strong><code>tf.GradientTape()</code></strong> idiom:</p>
<ul>
<li><p>Add <code>with tf.GradientTape() as tape:</code> block around our computations</p>
</li>
<li><p>Mark those tensors with respect to which we need to compute gradients by calling <code>tape.watch</code> (all variables are <strong>watched automatically</strong>)</p>
</li>
<li><p>Compute whatever we need (build computational graph)</p>
</li>
<li><p>Obtain gradients using <code>tape.gradient</code></p>
<p>a &#x3D; tf.random.normal(shape&#x3D;(2, 2))<br>b &#x3D; tf.random.normal(shape&#x3D;(2, 2))</p>
<p>with tf.GradientTape() as tape:<br>  tape.watch(a)  # Start recording the history of operations applied to <code>a</code><br>  c &#x3D; tf.sqrt(tf.square(a) + tf.square(b))  # Do some math using <code>a</code></p>
<h1 id="What’s-the-gradient-of-c-with-respect-to-a"><a href="#What’s-the-gradient-of-c-with-respect-to-a" class="headerlink" title="What’s the gradient of c with respect to a?"></a>What’s the gradient of <code>c</code> with respect to <code>a</code>?</h1><p>  dc_da &#x3D; tape.gradient(c, a)<br>  print(dc_da)</p>
</li>
</ul>
<p>监视变量、构建运算关系、计算梯度</p>
<h4 id="3-例1：线性回归问题"><a href="#3-例1：线性回归问题" class="headerlink" title="&lt; 3&gt;例1：线性回归问题"></a><strong>&lt; 3&gt;例1：线性回归问题</strong></h4><p>生成数据集</p>
<pre><code>import matplotlib.pyplot as plt
from sklearn.datasets import make_classification, make_regression
from sklearn.model_selection import train_test_split
import random

np.random.seed(13) # pick the seed for reproducability - change it to explore the effects of random variations

train_x = np.linspace(0, 3, 120)
train_labels = 2 * train_x + 0.9 + np.random.randn(*train_x.shape) * 0.5

plt.scatter(train_x,train_labels)
</code></pre>
<p>定义损失函数：</p>
<pre><code>input_dim = 1
output_dim = 1
learning_rate = 0.1

# This is our weight matrix
w = tf.Variable([[100.0]])
# This is our bias vector
b = tf.Variable(tf.zeros(shape=(output_dim,)))

def f(x):
  return tf.matmul(x,w) + b

def compute_loss(labels, predictions):
  return tf.reduce_mean(tf.square(labels - predictions))
</code></pre>
<p>训练函数：</p>
<pre><code>def train_on_batch(x, y):
  with tf.GradientTape() as tape:
    predictions = f(x)
    loss = compute_loss(y, predictions)
    # Note that `tape.gradient` works with a list as well (w, b).
    dloss_dw, dloss_db = tape.gradient(loss, [w, b])
  w.assign_sub(learning_rate * dloss_dw)
  b.assign_sub(learning_rate * dloss_db)
  return loss
</code></pre>
<p>训练集生成：</p>
<pre><code># Shuffle the data. 打乱数据
indices = np.random.permutation(len(train_x))
features = tf.constant(train_x[indices],dtype=tf.float32)
labels = tf.constant(train_labels[indices],dtype=tf.float32)
</code></pre>
<p>训练过程：（第 i 到 i+batch_size 为一组）</p>
<pre><code>batch_size = 4
for epoch in range(10):
  for i in range(0,len(features),batch_size):
    loss = train_on_batch(tf.reshape(features[i:i+batch_size],(-1,1)),tf.reshape(labels[i:i+batch_size],(-1,1)))
  print(&#39;Epoch %d: last batch loss = %.4f&#39; % (epoch, float(loss)))
</code></pre>
<p>绘制：</p>
<pre><code>plt.scatter(train_x,train_labels)
x = np.array([min(train_x),max(train_x)])
y = w.numpy()[0,0]*x+b.numpy()[0]
plt.plot(x,y,color=&#39;red&#39;)


We now have obtained optimized parameters $W$ and $b$. Note that their values are **similar to the original values used when generating the dataset** (W=2, b=1)
</code></pre>
<p>本文转自 <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_32971095/article/details/137124492">https://blog.csdn.net/qq_32971095/article/details/137124492</a>，如有侵权，请联系删除。</p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>SWQ
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://example.com/2024/03/29/AI%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="AI学习笔记">http://example.com/2024/03/29/AI学习笔记/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/opencv/" rel="tag"># opencv</a>
              <a href="/tags/c/" rel="tag"># c++</a>
              <a href="/tags/python/" rel="tag"># python</a>
              <a href="/tags/%E5%AD%A6%E4%B9%A0/" rel="tag"># 学习</a>
              <a href="/tags/%E7%AC%94%E8%AE%B0/" rel="tag"># 笔记</a>
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"># 人工智能</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/03/14/c%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AF%BC%E8%AE%BA%EF%BC%9C1%EF%BC%9E/" rel="prev" title="c语言实现进化算法——人工智能导论＜1＞">
      <i class="fa fa-chevron-left"></i> c语言实现进化算法——人工智能导论＜1＞
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/03/29/Vitis%20AI%E2%80%94%E2%80%94FPGA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="next" title="Vitis AI——FPGA学习笔记">
      Vitis AI——FPGA学习笔记 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC82MDQwMy8zNjg3Mw=="></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80-%E7%BB%ADopencv%E2%80%94%E2%80%94%E7%BA%A7%E8%81%94%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">1.</span> <span class="nav-text">一.续opencv——级联分类器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C-c%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">二.c语言实现进化算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89-LeNet-%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0-MNIST-%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB"><span class="nav-number">3.</span> <span class="nav-text">三.LeNet 网络实现 MNIST 手写数字识别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B-the-simplest-neural-network-model"><span class="nav-number">4.</span> <span class="nav-text">四.the simplest neural network model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94-Multi-Layered-Perceptron"><span class="nav-number">5.</span> <span class="nav-text">五.Multi-Layered Perceptron</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Gradient-Descent-Optimization%EF%BC%88%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%EF%BC%89"><span class="nav-number">5.1.</span> <span class="nav-text">1.Gradient Descent Optimization（梯度下降算法）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Multi-Layered-Perceptrons-and-Backpropagation%EF%BC%88%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%89"><span class="nav-number">5.2.</span> <span class="nav-text">2.Multi-Layered Perceptrons and Backpropagation（多层感知器和反向传播）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%8D%95%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.3.</span> <span class="nav-text">3.单层感知器模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8CX%E4%B8%BA%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%EF%BC%8CY%E4%B8%BA%E6%A0%87%E7%AD%BE%EF%BC%9A"><span class="nav-number">5.3.1.</span> <span class="nav-text">&lt; 1&gt;创建数据集，X为特征向量，Y为标签：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%EF%BC%9A"><span class="nav-number">5.3.2.</span> <span class="nav-text">**&lt; 2&gt; 前向传播计算过程： **</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8softmax%E5%87%BD%E6%95%B0%E8%BD%AC%E6%8D%A2%E4%B8%BA%E6%A6%82%E7%8E%87%EF%BC%9A"><span class="nav-number">5.3.3.</span> <span class="nav-text">&lt;3&gt;使用softmax函数转换为概率：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">5.3.4.</span> <span class="nav-text">&lt;4&gt;交叉熵损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Loss-Minimization-Problem-and-Network-Training%EF%BC%9A"><span class="nav-number">5.3.5.</span> <span class="nav-text">&lt;5&gt;Loss Minimization Problem and Network Training：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%87%BD%E6%95%B0%E5%B0%8F%E7%BB%93"><span class="nav-number">5.3.6.</span> <span class="nav-text">&lt;6&gt;函数小结</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-the-Model"><span class="nav-number">5.3.7.</span> <span class="nav-text">&lt;7&gt;Training the Model</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.4.</span> <span class="nav-text">4.网络模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C%E7%B1%BB-%EF%BC%9A"><span class="nav-number">5.4.1.</span> <span class="nav-text">&lt;1&gt;定义网络类 ：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multi-Layered-Models"><span class="nav-number">5.4.2.</span> <span class="nav-text">&lt;2&gt;Multi-Layered Models</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E4%BB%A3%E7%A0%81%E6%95%B4%E5%90%88"><span class="nav-number">5.5.</span> <span class="nav-text">5.代码整合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-layer-network-%E5%AE%9E%E7%8E%B0-mnist-%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB"><span class="nav-number">5.6.</span> <span class="nav-text">6.3-layer network 实现 mnist 手写数字识别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD-Neural-Network-Frameworks"><span class="nav-number">6.</span> <span class="nav-text">六.Neural Network Frameworks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Keras"><span class="nav-number">6.1.</span> <span class="nav-text">1.Keras</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-One-Layer-Network-Perceptron"><span class="nav-number">6.1.1.</span> <span class="nav-text">&lt;1&gt;Training One-Layer Network (Perceptron)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E2%91%A0%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89"><span class="nav-number">6.1.1.1.</span> <span class="nav-text">①模型定义</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Model-%E2%80%9Csequential%E2%80%9D-Layer-type-Output-Shape-Param"><span class="nav-number"></span> <span class="nav-text">Model: “sequential”_________________________________________________________________ Layer (type)                Output Shape              Param #</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E2%91%A1%E6%A8%A1%E5%9E%8B%E7%BC%96%E8%AF%91%EF%BC%88%E6%8C%87%E5%AE%9A%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E3%80%81%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E3%80%90%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AD%89%E3%80%91%E3%80%81%E7%B2%BE%E5%BA%A6%EF%BC%89"><span class="nav-number">0.0.0.1.</span> <span class="nav-text">②模型编译（指定损失函数、优化方法【梯度下降等】、精度）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E2%91%A2%E8%AE%AD%E7%BB%83"><span class="nav-number">0.0.0.2.</span> <span class="nav-text">③训练</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multi-Class-Classificatio%EF%BC%88%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%EF%BC%89"><span class="nav-number">0.0.1.</span> <span class="nav-text">&lt;2&gt;Multi-Class Classificatio（多分类问题）</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Two-ways-to-convert-to-one-hot-encoding"><span class="nav-number"></span> <span class="nav-text">Two ways to convert to one-hot encoding</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Multi-Label-Classification%EF%BC%88%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%EF%BC%89"><span class="nav-number">0.0.1.</span> <span class="nav-text">&lt;3&gt;Multi-Label Classification（多标签分类）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-Summary-of-Classification-Loss-Functions"><span class="nav-number">0.0.2.</span> <span class="nav-text">&lt;4&gt;总结 Summary of Classification Loss Functions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Tensorflow2-x-Keras"><span class="nav-number">0.1.</span> <span class="nav-text">2.Tensorflow2.x+Keras</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E5%BC%A0%E9%87%8F%E6%93%8D%E4%BD%9C"><span class="nav-number">0.1.1.</span> <span class="nav-text">&lt;1&gt;简单张量操作</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E2%91%A0%E5%88%9B%E5%BB%BA"><span class="nav-number">0.1.1.1.</span> <span class="nav-text">①创建</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E2%91%A1%E8%BF%90%E7%AE%97"><span class="nav-number">0.1.1.2.</span> <span class="nav-text">②运算</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6"><span class="nav-number">0.1.2.</span> <span class="nav-text">&lt;2&gt;计算梯度</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#What%E2%80%99s-the-gradient-of-c-with-respect-to-a"><span class="nav-number"></span> <span class="nav-text">What’s the gradient of c with respect to a?</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%BE%8B1%EF%BC%9A%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98"><span class="nav-number">0.0.1.</span> <span class="nav-text">&lt; 3&gt;例1：线性回归问题</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="SWQ"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">SWQ</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">41</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">27</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/xidianswq" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;xidianswq" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/3209507800@qq.com" title="E-Mail → 3209507800@qq.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/qq_32971095" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;qq_32971095" rel="noopener" target="_blank"><i class="fa fa-link fa-fw"></i>CSDN</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; Sat Feb 08 2025 08:00:00 GMT+0800 (中国标准时间) – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SWQ</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">790k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">11:58</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/three-waves.min.js"></script>


  















  

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

</body>
</html>
